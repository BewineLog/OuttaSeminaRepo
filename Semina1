수업 내용: 머신러닝을 위한 기초 수학
-R^n 공간과 함수
-행렬과 선형사상
-미분

*궁금했던 내용은 Outta 교재 또는 구글링을 통해 정리할 예정입니다.

수업을 들으면서 들었던 의문은 4가지 였다.
1. 선형사상의 성질은 알겠으나 선형 사상의 정의는 무엇인가?
2. 그래디언트가 중요한 이유는?
3, 연쇄법칙이 중요한 이유는?
3. 딥러닝에서의 최적화란 구체적으로 무엇을 의미하는가?


<1A>
선형사상이 갖는 정의는 '어떤 공간에서 새로운 벡터 공간으로 사상(->mapping)해주는 함수' 라고 한다.
그렇다면 왜 선형사상이 인공지능에서 중요한지가 의문이 들었는데,
2D나 3D로 모델링한 데이터에 대한 시야각을 다른 시야각으로 바꿀 때(from one viewing angle to the next), 선형사상이 되게 유용하다고 한다.

<2A>
그래디언트란 다변수 함수에서 모든 방향으로의 순간변화율을 의미한다.
그래디언트가 중요한 이유는 머신러닝 모델을 학습할 때 나오는데, 그래디언트는 모든 알고리즘에 잘 적용될 뿐만 아니라 오류의 변화율에 관련한 모든 가중치를 쉽게 측정할 수 있기에 중요하다고 한다.
이 부분은 아직 배우지 않은 경사하강법과 이어지는 것 같다.

<3A>
딥러닝에서 'Deep'이 의미하는 바는, 함수들이 많이 합성된 것을 의미하는데, 이 자체를 한번에 미분하려고 하면 너무 많은 연산과 시간이 소요 된다고 한다.
복잡한 함수를 여러 간단한 함수의 합성으로 보고 각각에 대해 미분을 하여 보다 간단하게 연산을 수행할 수 있기 때문에 중요하다고 한다.

<4A>
딥러닝에서 학습이 잘되었다는 것은, 모든 내용을 잘 담았다거나 하는 것이 아닌 손실함수의 값이 작다는 것을 의미한다고 한다. 
즉, 딥러닝의 목표는 손실함수를 최적화 하는 것이고, 이를 위해 함수의 값을 
최소화 혹은 최대화 하는 작업을 최적화 라고 한다고 한다.

출처:
1.https://deepai.org/machine-learning-glossary-and-terms/linear-transformation
2.https://builtin.com/data-science/gradient-descent
3. 수업교재- 인공지능 교육단체 OUTTA와 함께하는! 머신러닝 첫 단추 끼우기(Machine Learning with OUTTA
